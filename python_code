import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import jieba
import re
from random import shuffle
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures
from nltk.probability import FreqDist, ConditionalFreqDist

#df存储读取文件数据
df = pd.read_csv('train.csv')

#sub_list存储情感分类主题，共10类
sub_list = list(df['subject'].unique())

#sub_cnt_dict存储每一类对应的所有中文文本
sub_word_dict = dict(zip(sub_list, ['']*10))

#将文本中的中文提取出来
def Translate(str):
    pat = re.compile(u"[^\\u4e00-\\u9fa5]")
    result = ''.join(pat.split(str))
    return result

for item in sub_list:
    temp_content = ''.join(list(df[df['subject']==item]['content']))
    #jieba.cut 进行结巴分词工具分词
    sub_word_dict[item] = list(jieba.cut(Translate(temp_content),cut_all = False))
    
#print(sub_word_dict['价格'][:10])
#计算分词的卡方统计量，然后选取最高的number个
def Jieba_feature(sub_word_dict,number):
    #可统计所有词的词频
    word_fd = FreqDist()
    #可统计每个主题的词频
    con_word_fd = ConditionalFreqDist()
    #存储每个类别的word的总词数
    con_word_count = {}
    
    for sub in sub_word_dict.keys():
        for word in sub_word_dict[sub]:
            word_fd[word] += 1
            con_word_fd[sub][word] += 1
        temp_num = con_word_fd[sub].N()
        con_word_count[sub] = con_word_count.get(sub, temp_num)
    total_word_count = sum(con_word_count.values())
    
    word_scores = {} #存储每个词对应的信息量
    
    for word, fred in word_fd.items():
        word_scores[word] = 0
        for sub in sub_word_dict.keys():
            temp_num = BigramAssocMeasures.chi_sq(con_word_fd[sub][word], 
                        (fred,con_word_fd[sub].N()),
                        total_word_count)
            word_scores[word] += temp_num
    
    #把词按信息量进行排序，然后去前number个
    best_vals = sorted(word_scores.items(), 
                       key=lambda item:item[1],
                       reverse = True)[:number]
    best_words = set([w for w,s in best_vals])
    return dict([(word,True) for word in best_words])
    
#print(Jieba_feature(sub_word_dict, 10000))

#word_list 存储我们所取的作为特征的词
word_list = Jieba_feature(sub_word_dict, 1000)

#将数据集转换为特征数据和分类数据
def GetData(df, word_list,sub_list):
    train_list = []
    for index in range(df.shape[0]):
        temp_list = []
        word_vec = {}
        
        content = df['content'][index]
        subject = df['subject'][index]
        
        fen_ci = list(jieba.cut(Translate(content),cut_all = False))
        
        for word in fen_ci:
            if word in word_list:
                word_vec[word] = 'True'
                
        temp_list.append(word_vec)
        try:
            temp_list.append(sub_list.index(subject))
        except BaseException as e:
            temp_list.append(10)   #表示subject是一个空值，
        train_list.append(temp_list)
    return train_list


def GetValueData(data, df):    
    data2 = data.copy()
    for i in range(len(df['content_id'])):
        data2[i][1] = df['sentiment_value'][i]
    return data2

#获取可计算的数据集
data = GetData(df, word_list, sub_list)
#data2 = GetValueData(data, df) #有问题，运行之后，data  和data2 变成相同的了

#print(data2[0])
#shuffle(data)
#print(data2[0])
#打乱数据，然后将数据分为train and test
#shuffle(data)
#train_data = data[:6947]   #6947 条训练数据
#test_data = data[6947:]    #3000 条测试数据
##test_x, test_y = zip(*test_data)  #分离测试集的数据和标签

import sklearn
from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def Score(classifier, train_data, test_data):
    test_x, test_y = zip(*test_data)
    classifier = SklearnClassifier(classifier)
    classifier.train(train_data) #训练分类器
    pred = classifier.classify_many(test_x) #给出预测结果
    
    test_y = np.array(test_y)
    pred = np.array(pred)
    return sum(pred == test_y) / len(test_y)

shuffle(data)
train_data = data[:6947]   #6947 条训练数据
test_data = data[6947:] 

print('--------------------------subject-----------------------')
print('BernoulliNB`s accuracy is %f'  %Score(BernoulliNB(), train_data, test_data))
print('MultinomiaNB`s accuracy is %f'  %Score(MultinomialNB(), train_data, test_data))
print('LogisticRegression`s accuracy is  %f' %Score(LogisticRegression(), train_data, test_data))
print('SVC`s accuracy is %f'  %Score(SVC(), train_data, test_data))
print('LinearSVC`s accuracy is %f'  %Score(LinearSVC(), train_data, test_data))


#shuffle(data2)
#train_data2 = data2[:6947]   #6947 条训练数据
#test_data2 = data2[6947:]    #3000 条测试数据
#
#print('-----------------------sentiment_value---------------')
#print('BernoulliNB`s accuracy is %f'  %Score(BernoulliNB(), train_data2, test_data2))
#print('MultinomiaNB`s accuracy is %f'  %Score(MultinomialNB(), train_data2, test_data2))
#print('LogisticRegression`s accuracy is  %f' %Score(LogisticRegression(), train_data2, test_data2))
#print('SVC`s accuracy is %f'  %Score(SVC(),train_data2, test_data2))
#print('LinearSVC`s accuracy is %f'  %Score(LinearSVC(), train_data2, test_data2))



##开始预测比赛测试数据
#test_df = pd.read_csv('test_public.csv')  #比赛需要预测的数据
#test_df['subject'] = 0
#test_df['sentiment_value'] = 0
#test_data_sub = GetData(test_df, word_list,sub_list)
#test_data_value = GetValueData(test_data_sub, test_df)
#
##print(test_data_sub[0], test_data_value[0], sep = '\n')
##data  和  data2 分别为 test_data_sub,test_data_value的训练数据
##一个获取预测结果的预测函数
#def Predict(classifier, train_data, test_data):
#    test_x, test_y = zip(*test_data)
#    classifier = SklearnClassifier(classifier)
#    classifier.train(train_data) #训练分类器
#    pred = classifier.classify_many(test_x) #给出预测结果
#    return pred
#
#pre_sub = Predict(LogisticRegression(), data, test_data_sub)
#data2 = GetValueData(data, df)  #防止data因data2的改变而改变
#pre_value = Predict(LogisticRegression(), data2, test_data_value)
#
##print(pre_sub[:20], pre_value[:20])
#
#d = dict(zip(range(10), sub_list))
#f = lambda x:d.get(x,x)
#test_df['subject'] = pre_sub
#test_df['subject'] = test_df['subject'].map(f)
#test_df['sentiment_value'] = pre_value
##
#test_df.to_csv('pre_data.csv')
#



